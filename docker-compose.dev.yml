# TODO: need to add swagger, cadvisor integration with prometheus, as well as the backend integration for the grafana NodeJS APP Dashboard
# make devops, make frontend, make backend, make all

# ======================================================================================
# VOLUMES
# ======================================================================================

# Ill leave the volumes bind mounts as they are, until decided otherwise
volumes:
  backend_code_volume: # Using this name as it's in your current backend service
    name: ${PROJECT_NAME}_backend_code_volume
    driver: local
    driver_opts:
      o: bind
      device: ./backend
      type: none

  grafana_volume:
    name: ${PROJECT_NAME}_grafana_volume
    driver: local
    driver_opts:
      o: bind
      device: ./devops/external-volumes/grafana
      type: none

  prometheus_volume: 
    name: ${PROJECT_NAME}_prometheus_volume 
    driver: local
    driver_opts:
      o: bind
      device: ./devops/external-volumes/prometheus # Your existing config for prometheus data
      type: none

  sqlite_data:
    name: ${PROJECT_NAME}_sqlite_data
    driver: local
# --- NEW ELK STACK VOLUMES ---
  elasticsearch_data: # For Elasticsearch data persistence
    name: ${PROJECT_NAME}_elasticsearch_data
    driver: local
  # filebeat_data: # Optional: for Filebeat registry persistence
  #   name: ${PROJECT_NAME}_filebeat_data
  #   driver: local

# ======================================================================================
# NETWORKS
# ======================================================================================

networks:
  backend: # Network for application services (backend, potentially frontend)
    driver: bridge
  devops:  # Network for DevOps tooling (Prometheus, Grafana, Dockge, ELK)
    driver: bridge

services:

# ======================================================================================
# BACKEND
# ======================================================================================

  backend:
    container_name: ${PROJECT_NAME}_backend
    build:
      context: ./backend
      dockerfile: dockerfile
    volumes:
      - backend_code_volume:/usr/src/app # Mounts host's ./backend for code
      - /usr/src/app/node_modules      # Anonymous volume for node_modules
      - sqlite_data:/dbdata            # Mounts Docker volume for SQLite data
    networks:
      - backend  # Primary app network
      - devops   # For Prometheus scraping, and ELK Filebeat access if needed
    env_file:
      - ./backend/.env # DB_PATH should be /dbdata/database.db
    ports:
      - "${FASTIFY_HOST_PORT}:3000"
    restart: unless-stopped
    logging: # Explicitly define logging for Filebeat to pick up if configured for Docker logs
      driver: "json-file"
      options:
        max-size: "20m" # Increased size
        max-file: "5"
        tag: "{{.Name}}" # Tag with container name, useful in ELK

# ======================================================================================
# FRONTEND (Commented out as per your current file)
# ======================================================================================

  frontend:
    container_name: ${PROJECT_NAME}_frontend
    build:
      context: ./frontend # Points to your Next.js frontend directory
      dockerfile: Dockerfile # Uses frontend/Dockerfile (B-Line version)
    ports:
      # Exposes Next.js dev server (running on 3000 in container) to host
      # FRONTEND_HOST_PORT should be defined in your root .env (e.g., 8080)
      - "${FRONTEND_HOST_PORT:-8080}:3000"
    networks:
      - backend # Allows frontend (if making server-side calls or API routes) to reach 'backend' service
    # env_file:
    #   - ./frontend/.env # For NEXT_PUBLIC_ variables (if any, like API URLs)
    volumes:
       - ./frontend:/app
       # Anonymous volume to prevent host node_modules from overwriting container's
       # This ensures 'npm ci' in the Dockerfile is respected inside the container.
       - /app/node_modules
       # Anonymous volume for Next.js build cache/output during dev, good for performance
       - /app/.next
    # The CMD in the Dockerfile is already "npm run dev".
    # No 'command:' override needed here for B-line dev setup.
    restart: unless-stopped
    depends_on: # Optional, but good if frontend makes immediate server-side calls to backend on start
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}"

# ======================================================================================
# MANAGEMENT
# ======================================================================================

  dockge:
    container_name: ${PROJECT_NAME}_dockge
    image: louislam/dockge:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro # Read-only is safer
      - ./devops/external-volumes/dockge:/app/data
    ports:
      - "${DOCKGE_HOST_PORT:-5001}:5001"
    networks:
      - devops
    environment:
      - DOCKGE_STACKS_DIR=/app/data/stacks # Explicitly define if needed, usually defaults ok
      - DOCKGE_ENABLE_CONSOLE=${DOCKGE_ENABLE_CONSOLE:-true} # From your config
    restart: unless-stopped

# ======================================================================================
# MONITORING && DASHBOARD (Prometheus & Grafana)
# ======================================================================================

  prometheus:
    container_name: ${PROJECT_NAME}_prometheus
    image: prom/prometheus:${PROMETHEUS_IMAGE_TAG:-latest}
    volumes:
      - ./devops/configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_volume:/prometheus # Using your existing volume name for Prometheus data
    networks:
      - devops
      - backend # To scrape backend service
    ports:
      - "0.0.0.0:${PROMETHEUS_HOST_PORT}:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped

  grafana:
    container_name: ${PROJECT_NAME}_grafana
    image: grafana/grafana-oss:${GRAFANA_IMAGE_TAG:-latest}
    user: "0"
    volumes:
      - grafana_volume:/var/lib/grafana # Using your existing volume name for Grafana data
    networks:
      - devops
    ports:
      - "0.0.0.0:${GRAFANA_HOST_PORT}:3000"
    environment:
      - GF_SERVER_ROOT_URL=http://localhost:${GRAFANA_HOST_PORT}
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=${GF_PLUGINS_PREINSTALL:-grafana-clock-panel}
      - GF_LOG_LEVEL=${LOG_LEVEL:-info}
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASS:-admin}
      - GF_DATASOURCES_PATH=/etc/grafana/provisioning/datasources
    depends_on:
      prometheus:
        condition: service_started
    restart: unless-stopped

# ======================================================================================
# LOGGING - ELK STACK 
# ======================================================================================

  elasticsearch:
    container_name: ${PROJECT_NAME}_elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_VERSION}
    environment:
      - node.name=es01_chimera
      - cluster.name=chimera-elk-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=${ES_JAVA_OPTS:-Xms512m -Xmx512m}"
      - xpack.security.enabled=${ELASTICSEARCH_DISABLE_SECURITY:-false} # Default to false for ease, true for security
      - xpack.security.http.ssl.enabled=false # Assuming security disabled for now
      - xpack.security.transport.ssl.enabled=false # Assuming security disabled for now
      # - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-admin} # Add if security enabled
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile: { soft: 65535, hard: 65535 }
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      # - ./devops/configs/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro # Optional
    ports:
      - "${ELASTICSEARCH_HOST_PORT_HTTP}:9200"
      - "${ELASTICSEARCH_HOST_PORT_TRANSPORT}:9300" # For multiple nodes architecture
    networks: [devops]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -s --fail http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    container_name: ${PROJECT_NAME}_logstash
    image: docker.elastic.co/logstash/logstash:${ELASTIC_VERSION}
    volumes:
      - ./devops/configs/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./devops/configs/logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5044:5044" # For Filebeat input - now exposed for clarity, can be internal if Filebeat always in Docker
      # - "9600:9600" # Logstash monitoring API (optional)
    networks: [devops]
    environment:
      LS_JAVA_OPTS: "${LS_JAVA_OPTS:-Xms256m -Xmx256m}"
      # If xpack.security.enabled=true in Elasticsearch, Logstash needs credentials:
      # ELASTICSEARCH_USERNAME: logstash_internal # Or your chosen user
      # ELASTICSEARCH_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-changemeLogstashPassword}
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  kibana:
    container_name: ${PROJECT_NAME}_kibana
    image: docker.elastic.co/kibana/kibana:${ELASTIC_VERSION}
    volumes:
      - ./devops/configs/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    ports:
      - "${KIBANA_HOST_PORT}:5601"
    networks: [devops]
    environment:
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
      # If xpack.security.enabled=true in Elasticsearch, Kibana needs credentials:
      # ELASTICSEARCH_USERNAME: kibana_system # Default Kibana system user
      # ELASTICSEARCH_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-changemeKibanaPassword}
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  filebeat:
    container_name: ${PROJECT_NAME}_filebeat
    image: docker.elastic.co/beats/filebeat:${ELASTIC_VERSION}
    user: root # To access Docker logs and socket
    volumes:
      - ./devops/configs/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro # Mount Docker container logs dir from host
      - /var/run/docker.sock:/var/run/docker.sock:ro         # Mount Docker socket from host
      # - filebeat_data:/usr/share/filebeat/data # Optional: Persist Filebeat registry
    networks:
      - devops # To send to Logstash
      - backend # If reading specific app logs directly by path within another container's volume (more complex setup)
    command: ["--strict.perms=false"] # Often needed when running as root with mounted volumes
    depends_on:
      - logstash # Or elasticsearch if outputting directly
      # - backend # Ensure backend is started so its logs are available
    restart: unless-stopped


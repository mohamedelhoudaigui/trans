input {
  # Input from Filebeat
  beats {
    port => 5044
    # type => "docker" # You can assign a type here if needed for filtering
    # tags => ["beats_input"]
  }

  # Example: TCP input for direct JSON log shipping from applications (if not using Filebeat)
  # tcp {
  #   port => 5000
  #   codec => json_lines
  #   type => "app-json"
  # }
}

filter {
  # Attempt to parse JSON if the message looks like it
  # Filebeat's Docker input often sends logs as JSON strings within the 'message' field.
  if [message] =~ /^{.*}$/ {
    json {
      source => "message"
      # target => "parsed_json" # Store parsed JSON in a specific field
      # remove_field => ["message"] # Optionally remove original message if successfully parsed
    }
  }

  # If logs are not JSON, or after JSON parsing, you might want to use grok for unstructured data
  # This is a very generic grok pattern, adjust based on actual log formats.
  # It tries to capture common log elements.
  # if ![parsed_json] { # Only apply grok if JSON parsing didn't happen or target field not present
  #   grok {
  #     match => { "message" => "(?:%{TIMESTAMP_ISO8601:timestamp}|%{HTTPDATE:timestamp_http})?\s*\[?(?:%{LOGLEVEL:level}|%{WORD:level_alt})\]?\s*(?:\[%{DATA:thread}\])?\s*(?:%{JAVACLASS:class}|%{WORD:module})?\s*-\s*%{GREEDYDATA:log_message}" }
  #     # keep_empty_captures => false
  #     # overwrite => ["message"] # Overwrite original message with grokked log_message
  #     # add_tag => ["grokked"]
  #   }
  # }

  # Add container name as a top-level field if available from Filebeat's add_docker_metadata
  if [container][name] {
    mutate {
      add_field => { "container_name" => "%{[container][name]}" }
    }
  }
   if [agent][name] { # Filebeat often puts hostname in agent.name
    mutate {
      add_field => { "beat_host" => "%{[agent][name]}" }
    }
  }


  # Date filter to ensure @timestamp is correctly parsed from log lines if not set by Filebeat/input
  # if [timestamp] { # If your grok or JSON parsing extracted a 'timestamp' field
  #   date {
  #     match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "HTTPDATE" ]
  #     target => "@timestamp"
  #     # remove_field => ["timestamp"] # Clean up original timestamp field
  #   }
  # }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # Docker service name for Elasticsearch
    index => "chimera-logs-%{+YYYY.MM.dd}"   # Daily indices, prefixed with 'chimera-logs-'
    # manage_template => true
    # template_name => "chimera-logs"
    # template => "/usr/share/logstash/pipeline/chimera-logs-template.json" # If using a custom template
    # user => "${LOGSTASH_ES_USER}" # If Elasticsearch security is enabled
    # password => "${LOGSTASH_ES_PASSWORD}"
  }

  # For debugging the pipeline - prints events to Logstash's console
  # stdout {
  #   codec => rubydebug {
  #     metadata => true # Also show event metadata
  #   }
  # }
}
